# –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ ‚Ññ2 (–≤–∞—Ä–∏–∞–Ω—Ç 4)

## –¶–µ–ª—å –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã

–ü–æ –∑–∞–¥–∞–Ω–∏—é –≤—ã–±—Ä–∞—Ç—å —Å–≤–æ–∏ –∫–ª–∞—Å—Å—ã –∏ –æ–±—É—á–∏—Ç—å —Å–≤–µ—Ä—Ç–æ—á–Ω—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –∏–∑ –ø—Ä–∏–º–µ—Ä–∞, –∏—Å–ø–æ–ª—å–∑—É—è GPU, –∞ –∑–∞—Ç–µ–º –ø–æ–≤—ã—Å–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –ü—Ä–æ–≤–µ—Å—Ç–∏ —Ç—Ä–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è 3 —Ä–∞–∑–Ω—ã—Ö —Ç–∞–∫—Ç–∏–∫ –ø—É–ª–ª–∏–Ω–≥–∞: –ø—É–ª–ª–∏–Ω–≥ —Å –ø–æ–º–æ—â—å—é —à–∞–≥–∞ —Å–≤—ë—Ä—Ç–∫–∏ stride, –º–∞–∫—Å –ø—É–ª–ª–∏–Ω–≥, —É—Å—Ä–µ–¥–Ω—è—é—â–∏–π –ø—É–ª–ª–∏–Ω–≥. –°—Ä–∞–≤–Ω–∏—Ç—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∏ —Å—Ç–µ–ø–µ–Ω—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –í—ã–±—Ä–∞—Ç—å –ª—É—á—à—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Å—Ä–µ–¥—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è - —Ç–µ—Ä—è—é—Ç—Å—è –≤—Å–µ —Ç–µ–∫—É—é—â–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ.

–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ colab –≥–æ—Ç–æ–≤—É—é —É–∂–µ –æ–±—É—á–µ–Ω–Ω—É—é –Ω–∞ cifar100 –º–æ–¥–µ–ª—å. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ onnx –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ.

–°–∫–∞—á–∞—Ç—å –∫–∞—Ç–∞–ª–æ–≥ —Å html-—Ñ–∞–π–ª–æ–º –∏ –≤—Å—Ç—Ä–æ–∏—Ç—å –≤ –Ω–µ–≥–æ –¥–≤–∞ —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–µ–π - –æ–±—É—á–µ–Ω–Ω—É—é –Ω–∞ –õ–†1 –∏ –Ω–∞ –õ–†2.

–°–∫–∞—á–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –≤–∞—Ä–∏–∞–Ω—Ç—É –∏ –æ—Ç–∫—Ä—ã—Ç—å –∏—Ö –≤ html –ø–æ –∫–Ω–æ–ø–∫–µ. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤ —Å–∫—Ä–∏–ø—Ç–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.

–í—ã–±—Ä–∞—Ç—å –≤ js –Ω—É–∂–Ω—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è –≥–æ—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –æ–±–µ –º–æ–¥–µ–ª–∏, –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—É—é –∏ —Å–≤—ë—Ä—Ç–æ—á–Ω—É—é, –¥–≤–∏–≥–∞—è –∫–∞—Ä—Ç–∏–Ω–∫—É, —É–±–µ–¥–∏—Ç—å—Å—è –≤ –Ω–∞–ª–∏—á–∏–∏ —Å–≤–æ–π—Å—Ç–≤–∞ –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è.

---

## –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π CIFAR100

### –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫


```python
#!pip install torchsummary
import numpy as np
import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torchsummary import summary
import pickle
from sklearn.metrics import classification_report
from PIL import Image
from tqdm.auto import tqdm
from IPython.display import clear_output
import matplotlib.pyplot as plt
%matplotlib inline
```

### –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—É GPU, —á—Ç–æ–±—ã –Ω–∞ –Ω–µ–π —É—á–∏—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å


```python
!nvidia-smi
```

    Tue Nov 25 12:23:07 2025       
    +-----------------------------------------------------------------------------------------+
    | NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
    |-----------------------------------------+------------------------+----------------------+
    | GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
    |                                         |                        |               MIG M. |
    |=========================================+========================+======================|
    |   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |
    | N/A   52C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |
    |                                         |                        |                  N/A |
    +-----------------------------------------+------------------------+----------------------+
                                                                                             
    +-----------------------------------------------------------------------------------------+
    | Processes:                                                                              |
    |  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
    |        ID   ID                                                               Usage      |
    |=========================================================================================|
    |  No running processes found                                                             |
    +-----------------------------------------------------------------------------------------+
    


```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

### –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö CIFAR100


```python
!wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz
!tar -xvzf cifar-100-python.tar.gz
```

    --2025-11-25 12:23:07--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz
    Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30
    Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 169001437 (161M) [application/x-gzip]
    Saving to: ‚Äòcifar-100-python.tar.gz‚Äô
    
    cifar-100-python.ta 100%[===================>] 161.17M  44.1MB/s    in 4.1s    
    
    2025-11-25 12:23:12 (39.3 MB/s) - ‚Äòcifar-100-python.tar.gz‚Äô saved [169001437/169001437]
    
    cifar-100-python/
    cifar-100-python/file.txt~
    cifar-100-python/train
    cifar-100-python/test
    cifar-100-python/meta
    

### –ß—Ç–µ–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏


```python
with open('cifar-100-python/train', 'rb') as f:
    data_train = pickle.load(f, encoding='latin1')
with open('cifar-100-python/test', 'rb') as f:
    data_test = pickle.load(f, encoding='latin1')

CLASSES = [0, 33, 41]


train_X = data_train['data'].reshape(-1, 3, 32, 32)
train_X = np.transpose(train_X, [0, 2, 3, 1]) # NCWC -> NWHC
train_y = np.array(data_train['fine_labels'])
mask = np.isin(train_y, CLASSES)
train_X = train_X[mask].copy()
train_y = train_y[mask].copy()
train_y = np.unique(train_y, return_inverse=1)[1]
del data_train

test_X = data_test['data'].reshape(-1, 3, 32, 32)
test_X = np.transpose(test_X, [0, 2, 3, 1])
test_y = np.array(data_test['fine_labels'])
mask = np.isin(test_y, CLASSES)
test_X = test_X[mask].copy()
test_y = test_y[mask].copy()
test_y = np.unique(test_y, return_inverse=1)[1]
del data_test
Image.fromarray(train_X[50]).resize((256,256))
```




    
![output_15_0](https://github.com/user-attachments/assets/197edc45-3594-4046-b94a-8b6787fbf91a)

    



### –°–æ–∑–¥–∞–Ω–∏–µ Pytorch DataLoader'a


```python
batch_size = 128
dataloader = {}
for (X, y), part in zip([(train_X, train_y), (test_X, test_y)],
                        ['train', 'test']):
    tensor_x = torch.Tensor(X)
    tensor_y = F.one_hot(torch.Tensor(y).to(torch.int64),
                                     num_classes=len(CLASSES))/1.
    dataset = TensorDataset(tensor_x, tensor_y) # —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataloader[part] = DataLoader(dataset, batch_size=batch_size, shuffle=True) # —Å–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∫–ª–∞—Å—Å–∞ DataLoader
dataloader
```




    {'train': <torch.utils.data.dataloader.DataLoader at 0x7c71f7f22f30>,
     'test': <torch.utils.data.dataloader.DataLoader at 0x7c71fc093fb0>}



### –°–æ–∑–¥–∞–Ω–∏–µ Pytorch –º–æ–¥–µ–ª–∏ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏


```python
class Normalize(nn.Module):
    def __init__(self, mean, std):
        super(Normalize, self).__init__()
        self.mean = torch.tensor(mean).to(device)
        self.std = torch.tensor(std).to(device)

    def forward(self, input):
        x = input / 255.0
        x = x - self.mean
        x = x / self.std
        return x.permute(0, 3, 1, 2) # nhwc -> nm

class GlobalMaxPool2d(nn.Module):
    def __init__(self):
        super(GlobalMaxPool2d, self).__init__()

    def forward(self, input):
        out = F.adaptive_max_pool2d(input, output_size=1)
        return out.flatten(start_dim=1)

class Cifar100_CNN(nn.Module):
    def __init__(self, hidden_size=32, classes=100):
        super(Cifar100_CNN, self).__init__()
        # https://blog.jovian.ai/image-classification-of-cifar100-dataset-using-pytorch-8b7145242df1
        self.seq = nn.Sequential(
            Normalize([0.5074,0.4867,0.4411],[0.2011,0.1987,0.2025]),
            # –ø–µ—Ä–≤—ã–π —Å–ø–æ—Å–æ–± —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏ - —á–µ—Ä–µ–∑ stride
            nn.Conv2d(3, HIDDEN_SIZE, 5, stride=4, padding=2),
            nn.ReLU(),
            # –≤—Ç–æ—Ä–æ–π —Å–ø–æ—Å–æ–± —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏ - —á–µ—Ä–µ–∑ —Å–ª–æ–π –ø—É–ª–ª–∏–Ω–≥
            nn.Conv2d(HIDDEN_SIZE, HIDDEN_SIZE*2, 3, stride=1, padding=1),
            nn.ReLU(),
            nn.AvgPool2d(4),#nn.MaxPool2d(4),
            nn.Flatten(),
            nn.Linear(HIDDEN_SIZE*8, classes),
        )

    def forward(self, input):
        return self.seq(input)

HIDDEN_SIZE = 32
model = Cifar100_CNN(hidden_size=HIDDEN_SIZE, classes=len(CLASSES))
# NEW
model.to(device)
print(model(torch.rand(1, 32, 32, 3).to(device)))
summary(model, input_size=(32, 32, 3))
model
```

    tensor([[ 0.2622, -0.2338, -0.1270]], device='cuda:0',
           grad_fn=<AddmmBackward0>)
    ----------------------------------------------------------------
            Layer (type)               Output Shape         Param #
    ================================================================
             Normalize-1            [-1, 3, 32, 32]               0
                Conv2d-2             [-1, 32, 8, 8]           2,432
                  ReLU-3             [-1, 32, 8, 8]               0
                Conv2d-4             [-1, 64, 8, 8]          18,496
                  ReLU-5             [-1, 64, 8, 8]               0
             AvgPool2d-6             [-1, 64, 2, 2]               0
               Flatten-7                  [-1, 256]               0
                Linear-8                    [-1, 3]             771
    ================================================================
    Total params: 21,699
    Trainable params: 21,699
    Non-trainable params: 0
    ----------------------------------------------------------------
    Input size (MB): 0.01
    Forward/backward pass size (MB): 0.12
    Params size (MB): 0.08
    Estimated Total Size (MB): 0.22
    ----------------------------------------------------------------
    




    Cifar100_CNN(
      (seq): Sequential(
        (0): Normalize()
        (1): Conv2d(3, 32, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2))
        (2): ReLU()
        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (4): ReLU()
        (5): AvgPool2d(kernel_size=4, stride=4, padding=0)
        (6): Flatten(start_dim=1, end_dim=-1)
        (7): Linear(in_features=256, out_features=3, bias=True)
      )
    )



### –í—ã–±–æ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞


```python
criterion = nn.CrossEntropyLoss()
# –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è SGD c momentum
optimizer = optim.SGD(model.parameters(), lr=5e-3, momentum=0.9)
```

### –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ —ç–ø–æ—Ö–∞–º


```python
EPOCHS = 500
REDRAW_EVERY = 20
steps_per_epoch = len(dataloader['train'])
steps_per_epoch_val = len(dataloader['test'])
# NEW
pbar = tqdm(total=EPOCHS*steps_per_epoch)
losses = []
losses_val = []
passed = 0
for epoch in range(EPOCHS):  # –ø—Ä–æ—Ö–æ–¥ –ø–æ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑
    #running_loss = 0.0
    tmp = []
    model.train()
    for i, batch in enumerate(dataloader['train'], 0):
        # –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –º–∏–Ω–∏–±–∞—Ç—á–∞; batch —ç—Ç–æ –¥–≤—É—ç–ª–µ–º–µ–Ω—Ç–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑ [inputs, labels]
        inputs, labels = batch
        # –Ω–∞ GPU
        inputs, labels = inputs.to(device), labels.to(device)

        # –æ—á–∏—â–µ–Ω–∏–µ –ø—Ä–æ—à–ª—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —Å –ø—Ä–æ—à–ª–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
        optimizer.zero_grad()

        # –ø—Ä—è–º–æ–π + –æ–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥—ã + –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        #loss = F.cross_entropy(outputs, labels)
        loss.backward()
        optimizer.step()

        # –¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
        #running_loss += loss.item()
        accuracy = (labels.detach().argmax(dim=-1)==outputs.detach().argmax(dim=-1)).\
                    to(torch.float32).mean().cpu()*100
        tmp.append((loss.item(), accuracy.item()))
        pbar.update(1)
    #print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / steps_per_epoch:.3f}')
    losses.append((np.mean(tmp, axis=0),
                   np.percentile(tmp, 25, axis=0),
                   np.percentile(tmp, 75, axis=0)))
    #running_loss = 0.0
    tmp = []
    model.eval()
    with torch.no_grad(): # –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è
        for i, data in enumerate(dataloader['test'], 0):
            inputs, labels = data
            # –Ω–∞ GPU
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            #running_loss += loss.item()
            accuracy = (labels.argmax(dim=-1)==outputs.argmax(dim=-1)).\
                        to(torch.float32).mean().cpu()*100
            tmp.append((loss.item(), accuracy.item()))
    #print(f'[{epoch + 1}, {i + 1:5d}] val loss: {running_loss / steps_per_epoch_val:.3f}')
    losses_val.append((np.mean(tmp, axis=0),
                       np.percentile(tmp, 25, axis=0),
                       np.percentile(tmp, 75, axis=0)))
    if (epoch+1) % REDRAW_EVERY != 0:
        continue
    clear_output(wait=False)
    passed += pbar.format_dict['elapsed']
    pbar = tqdm(total=EPOCHS*steps_per_epoch, miniters=5)
    pbar.update((epoch+1)*steps_per_epoch)
    x_vals = np.arange(epoch+1)
    _, ax = plt.subplots(1, 2, figsize=(15, 5))
    stats = np.array(losses)
    stats_val = np.array(losses_val)
    ax[1].set_ylim(stats_val[:, 0, 1].min()-5, 100)
    ax[1].grid(axis='y')
    for i, title in enumerate(['CCE', 'Accuracy']):
        ax[i].plot(x_vals, stats[:, 0, i], label='train')
        ax[i].fill_between(x_vals, stats[:, 1, i],
                           stats[:, 2, i], alpha=0.4)
        ax[i].plot(x_vals, stats_val[:, 0, i], label='val')
        ax[i].fill_between(x_vals,
                           stats_val[:, 1, i],
                           stats_val[:, 2, i], alpha=0.4)
        ax[i].legend()
        ax[i].set_title(title)
    plt.show()
print('–û–±—É—á–µ–Ω–∏–µ –∑–∞–∫–æ–Ω—á–µ–Ω–æ –∑–∞ %s —Å–µ–∫—É–Ω–¥' % passed)
```


      0%|          | 0/6000 [00:00<?, ?it/s]


batch_size = 128

<img width="974" height="390" alt="image" src="https://github.com/user-attachments/assets/c21f4edd-172b-4b51-9a2e-40a96f53a368" />

batch_size = 64

<img width="974" height="377" alt="image" src="https://github.com/user-attachments/assets/6ecaa7d2-0ffc-4da1-a53c-ba1e1357559c" />

batch_size = 256

<img width="974" height="368" alt="image" src="https://github.com/user-attachments/assets/f010f325-90fe-45d0-998d-b797f3a3c9e2" />

stride

<img width="974" height="396" alt="image" src="https://github.com/user-attachments/assets/4ddc4cea-bd75-4bb3-85b7-c5257fadb754" />

Max-pooling

<img width="974" height="393" alt="image" src="https://github.com/user-attachments/assets/dba88987-fe0e-4da6-8f50-3a570dc8daf8" />

Average-pooling

<img width="974" height="390" alt="image" src="https://github.com/user-attachments/assets/ac625601-8d36-4f5b-a241-db71aea7f42c" />


### –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∞—Ö


```python
for part in ['train', 'test']:
    y_pred = []
    y_true = []
    with torch.no_grad(): # –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è
        for i, data in enumerate(dataloader[part], 0):
            inputs, labels = data
             # –Ω–∞ GPU
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs).detach().cpu().numpy()
            y_pred.append(outputs)
            y_true.append(labels.cpu().numpy())
        y_true = np.concatenate(y_true)
        y_pred = np.concatenate(y_pred)
        print(part)
        print(classification_report(y_true.argmax(axis=-1), y_pred.argmax(axis=-1),
                                    digits=4, target_names=list(map(str, CLASSES))))
        print('-'*50)
```

    train
                  precision    recall  f1-score   support
    
               0     1.0000    1.0000    1.0000       500
              33     1.0000    1.0000    1.0000       500
              41     1.0000    1.0000    1.0000       500
    
        accuracy                         1.0000      1500
       macro avg     1.0000    1.0000    1.0000      1500
    weighted avg     1.0000    1.0000    1.0000      1500
    
    --------------------------------------------------
    test
                  precision    recall  f1-score   support
    
               0     0.9490    0.9300    0.9394       100
              33     0.8958    0.8600    0.8776       100
              41     0.8585    0.9100    0.8835       100
    
        accuracy                         0.9000       300
       macro avg     0.9011    0.9000    0.9001       300
    weighted avg     0.9011    0.9000    0.9001       300
    
    --------------------------------------------------
    

---

## –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ ONNX


```python
# —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–≤—É–º—è —Å–ø–æ—Å–æ–±–∞–º–∏

# –ü–ï–†–í–´–ô –°–ü–û–°–û–ë: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (state_dict)
PATH = 'cifar_cnn.pth'

# —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
torch.save(model.state_dict(), PATH)

# –∑–∞–≥—Ä—É–∑–∫–∞
new_model = Cifar100_CNN(hidden_size=HIDDEN_SIZE, classes=len(CLASSES)).to(device)
new_model.load_state_dict(torch.load(PATH, map_location=device))
new_model.eval()


# –í–¢–û–†–û–ô –°–ü–û–°–û–ë: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Ü–µ–ª–∏–∫–æ–º
PATH2 = 'cifar_cnn.pt'

# —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
torch.save(model, PATH2)

# –∑–∞–≥—Ä—É–∑–∫–∞ (–≤–∞–∂–Ω–æ: weights_only=False –¥–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π PyTorch)
new_model_2 = torch.load(PATH2, map_location=device, weights_only=False)
new_model_2.eval()

```




    Cifar100_CNN(
      (seq): Sequential(
        (0): Normalize()
        (1): Conv2d(3, 32, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2))
        (2): ReLU()
        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (4): ReLU()
        (5): AvgPool2d(kernel_size=4, stride=4, padding=0)
        (6): Flatten(start_dim=1, end_dim=-1)
        (7): Linear(in_features=256, out_features=3, bias=True)
      )
    )




```python
!pip install onnx onnxscript
```

    Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.19.1)
    Collecting onnxscript
      Downloading onnxscript-0.5.6-py3-none-any.whl.metadata (13 kB)
    Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)
    Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)
    Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)
    Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.4)
    Collecting onnx_ir<2,>=0.1.12 (from onnxscript)
      Downloading onnx_ir-0.1.12-py3-none-any.whl.metadata (3.2 kB)
    Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (25.0)
    Downloading onnxscript-0.5.6-py3-none-any.whl (683 kB)
    [2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m683.0/683.0 kB[0m [31m18.0 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading onnx_ir-0.1.12-py3-none-any.whl (129 kB)
    [2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m129.3/129.3 kB[0m [31m8.7 MB/s[0m eta [36m0:00:00[0m
    [?25hInstalling collected packages: onnx_ir, onnxscript
    Successfully installed onnx_ir-0.1.12 onnxscript-0.5.6
    


```python
# –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –¥–ª—è –º–æ–¥–µ–ª–∏
x = torch.randn(1, 32, 32, 3, requires_grad=True).to(device)
torch_out = model(x)

# —ç–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏
torch.onnx.export(model,               # –º–æ–¥–µ–ª—å
                  x,                   # –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä (–∏–ª–∏ –∫–æ—Ä—Ç–µ–∂ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤)
                  "cifar100_CNN.onnx", # –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å (–ª–∏–±–æ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –ª–∏–±–æ fileObject)
                  export_params=True,  # —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ—Å–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–Ω—É—Ç—Ä–∏ —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏
                  opset_version=16,     # –≤–µ—Ä—Å–∏—è ONNX
                  do_constant_folding=True,  # —Å–ª–µ–¥—É–µ—Ç –ª–∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —É–∫–æ—Ä–∞—á–∏–≤–∞–Ω–∏–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                  input_names = ['input'],   # –∏–º—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è
                  output_names = ['output'],  # –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è
                  dynamic_axes={'input' : {0 : 'batch_size'},    # –¥–∏–Ω–∞–º–∏—á–Ω—ã–µ –æ—Å–∏, –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ —Ç–æ–ª—å–∫–æ —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞
                                'output' : {0 : 'batch_size'}})
```

 /tmp/ipython-input-1386216986.py:6: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.
  torch.onnx.export(model,               # –º–æ–¥–µ–ª—å
W1226 17:39:36.634000 2893 torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 16 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features
[torch.onnx] Obtain model graph for `Cifar100_CNN([...]` with `torch.export.export(..., strict=False)`...
[torch.onnx] Obtain model graph for `Cifar100_CNN([...]` with `torch.export.export(..., strict=False)`... ‚úÖ
[torch.onnx] Run decomposition...
WARNING:onnxscript.version_converter:The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 16).
[torch.onnx] Run decomposition... ‚úÖ
[torch.onnx] Translate the graph into ONNX...
[torch.onnx] Translate the graph into ONNX... ‚úÖ
ONNXProgram(
    model=
        <
            ir_version=10,
            opset_imports={'': 16},
            producer_name='pytorch',
            producer_version='2.9.0+cu126',
            domain=None,
            model_version=None,
        >
        graph(
            name=main_graph,
            inputs=(
                %"input"<FLOAT,[s31,32,32,3]>
            ),
            outputs=(
                %"output"<FLOAT,[1,3]>
            ),
            initializers=(
                %"seq.1.bias"<FLOAT,[32]>{TorchTensor(...)},
                %"seq.3.bias"<FLOAT,[64]>{TorchTensor(...)},
                %"seq.5.bias"<FLOAT,[128]>{TorchTensor(...)},
                %"seq.8.bias"<FLOAT,[3]>{TorchTensor<FLOAT,[3]>(Parameter containing: tensor([-0.3263,  0.3473, -0.0554], device='cuda:0', requires_grad=True), name='seq.8.bias')},
                %"seq.0.mean"<FLOAT,[3]>{TorchTensor<FLOAT,[3]>(tensor([0.5074, 0.4867, 0.4411], device='cuda:0'), name='seq.0.mean')},
                %"seq.0.std"<FLOAT,[3]>{TorchTensor<FLOAT,[3]>(tensor([0.2011, 0.1987, 0.2025], device='cuda:0'), name='seq.0.std')},
                %"seq.1.weight"<FLOAT,[32,3,5,5]>{TorchTensor(...)},
                %"seq.3.weight"<FLOAT,[64,32,3,3]>{TorchTensor(...)},
                %"seq.5.weight"<FLOAT,[128,64,3,3]>{TorchTensor(...)},
                %"seq.8.weight"<FLOAT,[3,512]>{TorchTensor(...)},
                %"val_4"<INT64,[2]>{Tensor<INT64,[2]>(array([  1, 512]), name='val_4')},
                %"val_0"<FLOAT,[]>{TensorProtoTensor<FLOAT,[]>(array(255., dtype=float32), name='val_0')}
            ),
        ) {
             0 |  # node_div
                  %"div"<FLOAT,[s31,32,32,3]> ‚¨ÖÔ∏è ::Div(%"input", %"val_0"{255.0})
             1 |  # node_sub_1
                  %"sub_1"<FLOAT,[1,32,32,3]> ‚¨ÖÔ∏è ::Sub(%"div", %"seq.0.mean"{[0.5073999762535095, 0.48669999837875366, 0.44110000133514404]})
             2 |  # node_div_1
                  %"div_1"<FLOAT,[1,32,32,3]> ‚¨ÖÔ∏è ::Div(%"sub_1", %"seq.0.std"{[0.20110000669956207, 0.19869999587535858, 0.20250000059604645]})
             3 |  # node_permute
                  %"permute"<FLOAT,[1,3,32,32]> ‚¨ÖÔ∏è ::Transpose(%"div_1") {perm=(0, 3, 1, 2)}
             4 |  # node_conv2d
                  %"conv2d"<FLOAT,[1,32,8,8]> ‚¨ÖÔ∏è ::Conv(%"permute", %"seq.1.weight"{...}, %"seq.1.bias"{...}) {group=1, pads=(2, 2, 2, 2), auto_pad='NOTSET', strides=(4, 4), dilations=(1, 1)}
             5 |  # node_relu
                  %"relu"<FLOAT,[1,32,8,8]> ‚¨ÖÔ∏è ::Relu(%"conv2d")
             6 |  # node_conv2d_1
                  %"conv2d_1"<FLOAT,[1,64,4,4]> ‚¨ÖÔ∏è ::Conv(%"relu", %"seq.3.weight"{...}, %"seq.3.bias"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}
             7 |  # node_relu_1
                  %"relu_1"<FLOAT,[1,64,4,4]> ‚¨ÖÔ∏è ::Relu(%"conv2d_1")
             8 |  # node_conv2d_2
                  %"conv2d_2"<FLOAT,[1,128,2,2]> ‚¨ÖÔ∏è ::Conv(%"relu_1", %"seq.5.weight"{...}, %"seq.5.bias"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}
             9 |  # node_relu_2
                  %"relu_2"<FLOAT,[1,128,2,2]> ‚¨ÖÔ∏è ::Relu(%"conv2d_2")
            10 |  # node__unsafe_view
                  %"_unsafe_view"<FLOAT,[1,512]> ‚¨ÖÔ∏è ::Reshape(%"relu_2", %"val_4"{[1, 512]}) {allowzero=1}
            11 |  # node_linear
                  %"output"<FLOAT,[1,3]> ‚¨ÖÔ∏è ::Gemm(%"_unsafe_view", %"seq.8.weight"{...}, %"seq.8.bias"{[-0.32634788751602173, 0.3472962975502014, -0.055435847491025925]}) {beta=1.0, transB=1, alpha=1.0, transA=0}
            return %"output"<FLOAT,[1,3]>
        }


    ,
    exported_program=
        ExportedProgram:
            class GraphModule(torch.nn.Module):
                def forward(self, p_seq_1_weight: "f32[32, 3, 5, 5]", p_seq_1_bias: "f32[32]", p_seq_3_weight: "f32[64, 32, 3, 3]", p_seq_3_bias: "f32[64]", p_seq_5_weight: "f32[128, 64, 3, 3]", p_seq_5_bias: "f32[128]", p_seq_8_weight: "f32[3, 512]", p_seq_8_bias: "f32[3]", c_seq_0_mean: "f32[3]", c_seq_0_std: "f32[3]", input: "f32[s31, 32, 32, 3]"):
                    input_1 = input
            
                     # File: /tmp/ipython-input-3043060065.py:8 in forward, code: x = input / 255.0
                    div: "f32[s31, 32, 32, 3]" = torch.ops.aten.div.Tensor(input_1, 255.0);  input_1 = None
            
                     # File: /tmp/ipython-input-3043060065.py:9 in forward, code: x = x - self.mean
                    sub_1: "f32[1, 32, 32, 3]" = torch.ops.aten.sub.Tensor(div, c_seq_0_mean);  div = c_seq_0_mean = None
            
                     # File: /tmp/ipython-input-3043060065.py:10 in forward, code: x = x / self.std
                    div_1: "f32[1, 32, 32, 3]" = torch.ops.aten.div.Tensor(sub_1, c_seq_0_std);  sub_1 = c_seq_0_std = None
            
                     # File: /tmp/ipython-input-3043060065.py:11 in forward, code: return x.permute(0, 3, 1, 2) # nhwc -> nm
                    permute: "f32[1, 3, 32, 32]" = torch.ops.aten.permute.default(div_1, [0, 3, 1, 2]);  div_1 = None
            
                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
                    conv2d: "f32[1, 32, 8, 8]" = torch.ops.aten.conv2d.default(permute, p_seq_1_weight, p_seq_1_bias, [4, 4], [2, 2]);  permute = p_seq_1_weight = p_seq_1_bias = None
            
                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
                    relu: "f32[1, 32, 8, 8]" = torch.ops.aten.relu.default(conv2d);  conv2d = None
            
                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
                    conv2d_1: "f32[1, 64, 4, 4]" = torch.ops.aten.conv2d.default(relu, p_seq_3_weight, p_seq_3_bias, [2, 2], [1, 1]);  relu = p_seq_3_weight = p_seq_3_bias = None
            
                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
                    relu_1: "f32[1, 64, 4, 4]" = torch.ops.aten.relu.default(conv2d_1);  conv2d_1 = None
            
                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
                    conv2d_2: "f32[1, 128, 2, 2]" = torch.ops.aten.conv2d.default(relu_1, p_seq_5_weight, p_seq_5_bias, [2, 2], [1, 1]);  relu_1 = p_seq_5_weight = p_seq_5_bias = None
            
                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
                    relu_2: "f32[1, 128, 2, 2]" = torch.ops.aten.relu.default(conv2d_2);  conv2d_2 = None
            
                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/flatten.py:56 in forward, code: return input.flatten(self.start_dim, self.end_dim)
                    clone: "f32[1, 128, 2, 2]" = torch.ops.aten.clone.default(relu_2, memory_format = torch.contiguous_format);  relu_2 = None
                    _unsafe_view: "f32[1, 512]" = torch.ops.aten._unsafe_view.default(clone, [1, 512]);  clone = None
            
                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                    linear: "f32[1, 3]" = torch.ops.aten.linear.default(_unsafe_view, p_seq_8_weight, p_seq_8_bias);  _unsafe_view = p_seq_8_weight = p_seq_8_bias = None
                    return (linear,)
            
        Graph signature: 
            # inputs
            p_seq_1_weight: PARAMETER target='seq.1.weight'
            p_seq_1_bias: PARAMETER target='seq.1.bias'
            p_seq_3_weight: PARAMETER target='seq.3.weight'
            p_seq_3_bias: PARAMETER target='seq.3.bias'
            p_seq_5_weight: PARAMETER target='seq.5.weight'
            p_seq_5_bias: PARAMETER target='seq.5.bias'
            p_seq_8_weight: PARAMETER target='seq.8.weight'
            p_seq_8_bias: PARAMETER target='seq.8.bias'
            c_seq_0_mean: CONSTANT_TENSOR target='seq.0.mean'
            c_seq_0_std: CONSTANT_TENSOR target='seq.0.std'
            input: USER_INPUT
    
            # outputs
            linear: USER_OUTPUT
    
        Range constraints: {s31: VR[0, 2]}

)

---

## Loss landscape

### –∏–¥–µ—è https://arxiv.org/abs/1712.09913


```python
from copy import deepcopy
state_dict_backup = deepcopy(model.state_dict())
```


```python
def generate_theta(seed=None):
    model.eval()
    model.load_state_dict(state_dict_backup)
    if seed is not None:
        np.random.seed(seed)
    params = []
    with torch.no_grad():
        for w in model.parameters():
            params.append(w.detach().cpu().numpy())
    params_n = np.concatenate([p.flatten() for p in params]).size
    random_theta_flat = np.random.normal(size=params_n)
    random_theta = []
    offset = 0
    for p in params:
        p_weights = p.flatten().size
        random_theta.append(random_theta_flat[offset:offset+p_weights].\
                            reshape(*p.shape))
        # normalization
        rank = random_theta[-1].shape.__len__()
        if rank == 4: # convolution
            #print('Conv')
            # Frobenius norm
            norm_r = np.sqrt((random_theta[-1]**2).sum(axis=-1).sum(axis=-1))
            norm_p = np.sqrt((p**2).sum(axis=-1).sum(axis=-1))
            norm = (norm_p / norm_r).reshape(*norm_p.shape, 1, 1)
        elif rank == 2: # fully connected
            #print('FC')
            norm_r = np.sqrt((random_theta[-1]**2).sum(axis=-1))
            norm_p = np.sqrt((p**2).sum(axis=-1))
            norm = (norm_p / norm_r).reshape(-1, 1)
        elif rank == 1: # bias
            #print('bias')
            norm_r = np.sqrt((random_theta[-1]**2).sum())
            norm_p = np.sqrt((p**2).sum())
            norm = norm_p / norm_r
        random_theta[-1] = random_theta[-1]*norm
        offset += p_weights
    assert offset==params_n, \
            "Not all params are utilized. Expected %d, found %d"%(params_n, offset)
    return random_theta

theta1 = generate_theta(seed=0)
theta2 = generate_theta(seed=1011)
```


```python
criterion2 = nn.CrossEntropyLoss(reduction='none')
loss_curve = []
alphas = np.arange(-1500, 1500, step=5)/1000
for alpha in tqdm(alphas):
    state_dict = model.state_dict()
    for (k, v), v_new in zip(state_dict_backup.items(), theta1):
        tensor = v.clone().detach() + \
              alpha*(torch.tensor(v_new, device=device) - v.clone().detach())
        state_dict[k] = tensor
    model.load_state_dict(state_dict)
    loss = []

    with torch.no_grad(): # –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è
        for i, data in enumerate(dataloader['test'], 0):
            inputs, labels = data
            # –Ω–∞ GPU
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs).detach()
            loss.append(criterion2(outputs, labels).detach().cpu().numpy())
    loss_curve.append(np.concatenate(loss).mean())
```


      0%|          | 0/600 [00:00<?, ?it/s]



```python
plt.plot(alphas, loss_curve)
plt.yscale('log')
plt.xlabel('alpha')
plt.ylabel('CCE, log10')
```




    Text(0, 0.5, 'CCE, log10')




    
<img width="570" height="432" alt="output_35_1" src="https://github.com/user-attachments/assets/51bba262-99e4-4003-b29f-0724d600f97c" />

    


### 2D


```python
# —É–≤–µ–ª–∏—á–∏–≤ step, –º–æ–∂–Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
# –æ–¥–Ω–∞–∫–æ –≤–º–µ—Å—Ç–µ —Å —ç—Ç–∏–º —Ç–µ—Ä—è–µ—Ç—Å—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
alphas = []
vals = np.arange(-150, 151, step=5)/100
size = vals.size

# —Å–æ–∑–¥–∞—ë–º –∑–∞–¥–∞–Ω–Ω—ã–π —Ä–∞—Å—Ç—Ä
for a1 in vals:
    for a2 in vals:
        alphas.append((a1, a2))

alphas = np.array(alphas)
```


```python
Z = []
for a1, a2 in tqdm(alphas):
    state_dict = model.state_dict()
    for (k, v), v_new, v_new2 in zip(state_dict_backup.items(), theta1, theta2):
        # –ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è
        tensor = v.clone().detach() + \
                 a1*(torch.tensor(v_new, device=device) - \
                     v.clone().detach())
        # –µ—â—ë —Ä–∞–∑ —Å–æ –≤—Ç–æ—Ä—ã–º –≤–µ–∫—Ç–æ—Ä–æ–º
        tensor = tensor + a2*(torch.tensor(v_new2, device=device) - tensor)
        state_dict[k] = tensor
        #print(k, tensor, v_new)
    model.load_state_dict(state_dict)
    loss = []
    with torch.no_grad(): # –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è
        for i, data in enumerate(dataloader['test'], 0):
            inputs, labels = data
            # –Ω–∞ GPU
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs).detach()#.cpu().numpy()
            loss.append(criterion2(outputs, labels).detach().cpu().numpy())
    Z.append(np.concatenate(loss).mean())
ZZ = np.array(Z)
```


      0%|          | 0/3721 [00:00<?, ?it/s]



```python
# –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –≥—Ä–∞—Ñ–∏–∫–∞
plt.figure(figsize=(8, 8))
# –æ—Ç—Ä–∏—Å–æ–≤–∫–∞ –∑–∞–∫—Ä–∞—à–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç—É—Ä–æ–≤, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ 2 —á–∞—Å—Ç–∏ 1 –õ–†
cs = plt.contourf(alphas[:,0].reshape(size, size),
             alphas[:,1].reshape(size, size),
             np.log10(ZZ.reshape(size, size)),
             levels=255,
             cmap=plt.cm.jet,
             )
# —É—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ü–≤–µ—Ç–æ–≤–æ–π —à–∫–∞–ª—ã –∏ –µ—ë –Ω–∞–∑–≤–∞–Ω–∏—è
plt.colorbar(cs).ax.set_ylabel('CCE, log10', rotation=270, fontsize=15)
# —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Å—è–º X, Y
plt.xlabel('Œ±', fontsize=20)
plt.ylabel('Œ≤', fontsize=20)
plt.show()
```


    
<img width="712" height="698" alt="output_39_0" src="https://github.com/user-attachments/assets/8b26abd9-7378-4cf7-ae79-eb402907e8fa" />

    


### 3D


```python
fig, ax = plt.subplots(figsize=(8, 8), subplot_kw={"projection": "3d"})
# —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Å—è–º X, Y –∏ Z
ax.set_xlabel('Œ±', fontsize=20)
ax.set_ylabel('Œ≤', fontsize=20)
ax.set_zlabel('CCE, log10', fontsize=20)
# –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ —Å–µ—Ç–∫–∏, –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã
ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
# –æ—Ç—Ä–∏—Å–æ–≤–∫–∞ 3D –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏, –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã contourf
surf = ax.plot_surface(alphas[:,0].reshape(size, size),
                       alphas[:,1].reshape(size, size),
                       np.log10(ZZ.reshape(size, size)),
                       cmap=plt.cm.coolwarm,
                       linewidth=0.1,
                       edgecolors='k',
                       alpha=0.8,
                       antialiased=True)
# –ø–µ—Ä–≤—ã–π –∞–≥—Ä—É–º–µ–Ω—Ç - –≤—Ä–∞—â–µ–Ω–∏–µ –≤–æ–∫—Ä—É–≥ XY, –≤—Ç–æ—Ä–æ—Ä–π –∞—Ä–≥—É–º–µ–Ω—Ç - –≤–æ–∫—Ä—É–≥ YZ
ax.view_init(40, -30)
```



<img width="658" height="645" alt="output_41_0" src="https://github.com/user-attachments/assets/192e108d-c6d3-44cd-9139-6ff7ae16356b" />



---

## –û—Ç–≤–µ—Ç—ã –Ω–∞ –∑–∞–¥–∞–Ω–∏—è –¥–ª—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã (–≤–∞—Ä–∏–∞–Ω—Ç 4)

_–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –¥–ª—è 3 –∫–ª–∞—Å—Å–æ–≤ CIFAR-100: 0, 33 –∏ 41 (–≥—Ä—É–ø–ø–∞ 22, –≤–∞—Ä–∏–∞–Ω—Ç 4)._

**1. –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π –º–æ–¥–µ–ª–∏**

–ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (CCE) –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ —É–±—ã–≤–∞–µ—Ç –∏ –∫ –∫–æ–Ω—Ü—É –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä–µ–º–∏—Ç—Å—è –∫ –Ω—É–ª—é. –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ 100%. –ù–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ loss —Å–Ω–∞—á–∞–ª–∞ –±—ã—Å—Ç—Ä–æ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è, –∑–∞—Ç–µ–º –ø–æ—Å–ª–µ ~100‚Äì150-–π —ç–ø–æ—Ö–∏ –≤—ã—Ö–æ–¥–∏—Ç –Ω–∞ –ø–ª–∞—Ç–æ –∏ –Ω–∞—á–∏–Ω–∞–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–æ —Ä–∞—Å—Ç–∏, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ —Ç–æ—á–Ω–æ—Å—Ç—å –∫–æ–ª–µ–±–ª–µ—Ç—Å—è –≤ —Ä–∞–π–æ–Ω–µ 88‚Äì92%. –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–∫–æ–ª–æ 90%. –¢–∞–∫–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ (–æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train –∏ –∑–∞–º–µ—Ç–Ω–æ –º–µ–Ω—å—à–∞—è –Ω–∞ val/test, —Ä–æ—Å—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ loss –ø—Ä–∏ –¥–∞–ª—å–Ω–µ–π—à–µ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ) —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç –æ –ø–æ—è–≤–ª–µ–Ω–∏–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø–æ–∑–¥–Ω–∏—Ö —ç–ø–æ—Ö–∞—Ö, —Ö–æ—Ç—è –≤ —Ü–µ–ª–æ–º –º–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ —Ä–∞–∑–ª–∏—á–∞–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã.

**2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç—Ä–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –ø—É–ª–ª–∏–Ω–≥–∞**

–ë—ã–ª–∏ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω—ã —Ç—Ä–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å—É–±–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏: max-pooling, average-pooling –∏ stride.  
Max-pooling –¥–∞–ª –Ω–∞–∏–ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: –ø—Ä–∏ –Ω—ë–º —Å–µ—Ç—å –ª—É—á—à–µ –≤—ã–¥–µ–ª—è–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —è—Ä–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≥—Ä–∞–Ω–∏—Ü—ã, —É–≥–ª—ã, –ø—è—Ç–Ω–∞ —Ç–µ–∫—Å—Ç—É—Ä—ã), –∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –∫–ª–∞—Å—Å—É. Average-pooling —Å–≥–ª–∞–∂–∏–≤–∞–µ—Ç –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —É—Å—Ä–µ–¥–Ω—è–µ—Ç –∫–∞–∫ –ø–æ–ª–µ–∑–Ω—ã–µ, —Ç–∞–∫ –∏ —à—É–º–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, –ø–æ—ç—Ç–æ–º—É —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–µ–º–Ω–æ–≥–æ –Ω–∏–∂–µ. Stride –ø–æ–∫–∞–∑–∞–ª –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –æ–∫–∞–∑–∞–ª—Å—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–µ–µ –∫ –≤—ã–±—Ä–æ—Å–∞–º. –í –∏—Ç–æ–≥–µ –Ω–∞–∏–±–æ–ª–µ–µ –≤—ã–≥–æ–¥–Ω—ã–º –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–º –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –ø—Ä–æ—Å—Ç–æ—Ç–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–∫–∞–∑–∞–ª—Å—è max-pooling, –∫–æ—Ç–æ—Ä—ã–π –∏ –±—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ.

**3. –í–ª–∏—è–Ω–∏–µ —á–∏—Å–ª–∞ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–≤–µ—Ä—Ç–∫–∏**

–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—ë–≤ —Å –æ–¥–Ω–æ–≥–æ –¥–æ –¥–≤—É—Ö –ø–æ–∑–≤–æ–ª–∏–ª–æ —Å–µ—Ç–∏ –≤—ã–¥–µ–ª—è—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö –≥—Ä–∞–Ω–∏—Ü –∫ –±–æ–ª–µ–µ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–º –∫–æ–Ω—Ç—É—Ä–∞–º –∏ —Ç–µ–∫—Å—Ç—É—Ä–∞–º), —á—Ç–æ –ø—Ä–∏–≤–µ–ª–æ –∫ –∑–∞–º–µ—Ç–Ω–æ–º—É —Ä–æ—Å—Ç—É —Ç–æ—á–Ω–æ—Å—Ç–∏. –ü–æ–ø—ã—Ç–∫–∞ –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≥–ª—É–±–∏–Ω—ã –ø—Ä–∏–≤–æ–¥–∏–ª–∞ –∫ —Ä–æ—Å—Ç—É –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è –∏ —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç–∏ –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é –ø—Ä–∏ –Ω–µ—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –≤—ã–∏–≥—Ä—ã—à–µ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É.  
–†–∞–∑–º–µ—Ä —è–¥—Ä–∞ 3√ó3 –ø–æ–∫–∞–∑–∞–ª —Å–µ–±—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º: –ø—Ä–∏ —Å–ª–∏—à–∫–æ–º –∫—Ä—É–ø–Ω–æ–º —è–¥—Ä–µ (5√ó5 –∏ –±–æ–ª–µ–µ) –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ä–∞—Å—Ç—ë—Ç, –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ —Ä–∞–∑–º—ã–≤–∞—é—Ç—Å—è –∏ –∫–∞—á–µ—Å—Ç–≤–æ –ø–∞–¥–∞–µ—Ç; –ø—Ä–∏ –º–µ–Ω—å—à–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º receptive field —Å–µ—Ç—å —Ö—É–∂–µ —É–ª–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —à–∞–≥–∞ —Å–≤—ë—Ä—Ç–∫–∏ (stride) —É–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ, –Ω–æ –ø—Ä–∏ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–º —à–∞–≥–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç–µ—Ä—è–µ—Ç—Å—è, –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –ø–∞–¥–∞–µ—Ç. –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ padding –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ç–æ–º—É, —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∫—Ä–∞—è–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Ö—É–∂–µ; –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ¬´same¬ª-padding (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞ –∫–∞—Ä—Ç—ã) –¥–∞—ë—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ.

**4. –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏**

–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –≤ –¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

- –æ—à–∏–±–∫–∞ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —É–±—ã–≤–∞—Ç—å –ø–æ—á—Ç–∏ –¥–æ –Ω—É–ª—è;
- –æ—à–∏–±–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —á–∏—Å–ª–∞ —ç–ø–æ—Ö –ø–µ—Ä–µ—Å—Ç–∞—ë—Ç —É–±—ã–≤–∞—Ç—å –∏ –Ω–∞—á–∏–Ω–∞–µ—Ç –ø–ª–∞–≤–Ω–æ —Ä–∞—Å—Ç–∏;
- —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 100%, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–µ –æ—Å—Ç–∞—ë—Ç—Å—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∏–∂–µ (~90%).

–î–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–∂–Ω–æ:

- —Å–æ–∫—Ä–∞—Ç–∏—Ç—å —á–∏—Å–ª–æ —ç–ø–æ—Ö –∏ –ø—Ä–∏–º–µ–Ω—è—Ç—å —Ä–∞–Ω–Ω—é—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É –ø–æ –º–∏–Ω–∏–º—É–º—É val-loss;
- –¥–æ–±–∞–≤–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é (L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è / weight decay) –∏/–∏–ª–∏ Dropout –≤ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π;
- —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤ –∏/–∏–ª–∏ —á–∏—Å–ª–æ —Å–ª–æ—ë–≤);
- —Ä–∞—Å—à–∏—Ä–∏—Ç—å –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É –∑–∞ —Å—á—ë—Ç –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π (—Å–ª—É—á–∞–π–Ω—ã–µ –ø–æ–≤–æ—Ä–æ—Ç—ã, —Å–¥–≤–∏–≥–∏, –æ—Ç—Ä–∞–∂–µ–Ω–∏—è, –Ω–µ–±–æ–ª—å—à–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —è—Ä–∫–æ—Å—Ç–∏).

**5. –í–ª–∏—è–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è**

–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ —ç–ø–æ—Ö —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ä–æ—Å—Ç—É —Ç–æ—á–Ω–æ—Å—Ç–∏, –æ–¥–Ω–∞–∫–æ –ø–æ—Å–ª–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —É—Ä–æ–≤–Ω—è (–ø–æ—Ä—è–¥–∫–∞ —Å–æ—Ç–Ω–∏ —ç–ø–æ—Ö) –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –ø–µ—Ä–µ—Å—Ç–∞—ë—Ç –∑–∞–º–µ—Ç–Ω–æ —É–ª—É—á—à–∞—Ç—å—Å—è, –∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ —É—Å–∏–ª–∏–≤–∞–µ—Ç—Å—è. –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –æ–∫–∞–∑–∞–ª—Å—è –¥–∏–∞–ø–∞–∑–æ–Ω —ç–ø–æ—Ö, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º val-loss –º–∏–Ω–∏–º–∞–ª–µ–Ω.  
–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ batch_size –ø–æ–∫–∞–∑–∞–ª–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å: –ø—Ä–∏ –º–∞–ª–æ–º batch (–Ω–∞–ø—Ä–∏–º–µ—Ä, 32) –æ–±—É—á–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–µ–µ, –Ω–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –±–æ–ª–µ–µ ¬´—à—É–º–Ω—ã–µ¬ª –∏ –ª—É—á—à–µ –≤—ã—Ö–æ–¥—è—Ç –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤; –ø—Ä–∏ –±–æ–ª—å—à–æ–º batch (128‚Äì256) –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª–µ–µ –≥–ª–∞–¥–∫–æ–µ, –Ω–æ –º–∏–Ω–∏–º—É–º–∞ –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ.  
–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate) —Å–∏–ª—å–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å: –ø—Ä–∏ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–º –∑–Ω–∞—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª—å —Ä–∞—Å—Ö–æ–¥–∏—Ç—Å—è –∏–ª–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ ¬´—Å–∫–∞—á–µ—Ç¬ª; –ø—Ä–∏ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ–º ‚Äî —Å—Ö–æ–¥–∏—Ç—Å—è –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ –∏ –º–æ–∂–µ—Ç –∑–∞ –æ—Ç–≤–µ–¥—ë–Ω–Ω–æ–µ —á–∏—Å–ª–æ —ç–ø–æ—Ö –Ω–µ –≤—ã–π—Ç–∏ –Ω–∞ —Ö–æ—Ä–æ—à–∏–π –º–∏–Ω–∏–º—É–º. –ù–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–∞–ª–∏ —É–º–µ—Ä–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è learning rate —Å –≤–æ–∑–º–æ–∂–Ω—ã–º –ø–æ—ç—Ç–∞–ø–Ω—ã–º —É–º–µ–Ω—å—à–µ–Ω–∏–µ–º –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è.

**6. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–π –∏ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π**

–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è —Å–µ—Ç—å, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —Ç–µ—Ö –∂–µ —Ç—Ä—ë—Ö –∫–ª–∞—Å—Å–∞—Ö, –ø–æ–∫–∞–∑–∞–ª–∞ –∑–∞–º–µ—Ç–Ω–æ –±–æ–ª–µ–µ –Ω–∏–∑–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Å–∏–ª—å–Ω–µ–µ —Å–∫–ª–æ–Ω–Ω–∞ –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ —á–∏—Å—Ç–æ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –≤–µ–ª–∏–∫–æ, —á—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª–µ–µ –∂—ë—Å—Ç–∫–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è —Å–µ—Ç—å –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: –∫–∞–∂–¥—ã–π –ø–∏–∫—Å–µ–ª—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Å–≤–æ–µ–≥–æ –ø–æ–ª–æ–∂–µ–Ω–∏—è.  
–°–≤–µ—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –Ω–∞–ø—Ä–æ—Ç–∏–≤, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ä–µ—Ü–µ–ø—Ç–∏–≤–Ω—ã–µ –ø–æ–ª—è –∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ –æ–±—É—á–∞–µ–º—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω—ã–µ –∫ —Å–¥–≤–∏–≥–∞–º –∏ –Ω–µ–±–æ–ª—å—à–∏–º –ø–æ–≤–æ—Ä–æ—Ç–∞–º –ø—Ä–∏–∑–Ω–∞–∫–∏. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ CNN –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç–µ—Å—Ç–µ, –±—ã—Å—Ç—Ä–µ–µ —Å—Ö–æ–¥–∏—Ç—Å—è –∏ –æ—Å—Ç–∞—ë—Ç—Å—è –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–æ–π –∫ –Ω–µ–±–æ–ª—å—à–∏–º –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏—è–º –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

**7. –î–µ–π—Å—Ç–≤–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —Å–∏–ª—å–Ω–µ–µ –≤—Å–µ–≥–æ –ø–æ–≤—ã—Å–∏–ª–∏ —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏**

–ù–∞–∏–±–æ–ª—å—à–∏–π –≤–∫–ª–∞–¥ –≤ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–Ω–µ—Å–ª–∏ —Å–ª–µ–¥—É—é—â–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è:

1. **–ü–µ—Ä–µ—Ö–æ–¥ –æ—Ç –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∫ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π** ‚Äî –ø–æ–∑–≤–æ–ª–∏–ª —Ä–µ–∑–∫–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º —Ä–æ—Å—Ç–µ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ —É—á—ë—Ç–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö.
2. **–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Ç–æ—Ä–æ–≥–æ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ –∫–∞–Ω–∞–ª–æ–≤** ‚Äî –¥–∞–ª–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ–∫–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —É–ª—É—á—à–∏–ª–æ —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤.
3. **–í—ã–±–æ—Ä –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ –ø—É–ª–ª–∏–Ω–≥–∞ –∏ –µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤** (max-pooling —Å –ø–æ–¥—Ö–æ–¥—è—â–∏–º —Ä–∞–∑–º–µ—Ä–æ–º –æ–∫–Ω–∞ –∏ —à–∞–≥–æ–º) ‚Äî –æ–±–µ—Å–ø–µ—á–∏–ª –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —É–º–µ–Ω—å—à–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π.
4. **–¢—â–∞—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è** (learning rate, —á–∏—Å–ª–æ —ç–ø–æ—Ö, —Ä–∞–∑–º–µ—Ä batch) ‚Äî –ø–æ–∑–≤–æ–ª–∏–ª–∞ –¥–æ–±–∏—Ç—å—Å—è —Ö–æ—Ä–æ—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –±–µ–∑ —Å–∏–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.
5. **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö** ‚Äî —É—Å–∫–æ—Ä–∏–ª–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Å–¥–µ–ª–∞–ª–∞ –ø—Ä–æ—Ü–µ—Å—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–º.

–í —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç–∏ —ç—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª–∏–ª–∏ –ø–æ–ª—É—á–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ—Ä—è–¥–∫–∞ 90% –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –¥–ª—è –≤–∞—Ä–∏–∞–Ω—Ç–∞ 4 –ø—Ä–∏ –ø—Ä–∏–µ–º–ª–µ–º–æ–º –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∫ –Ω–µ–±–æ–ª—å—à–∏–º –≤–∞—Ä–∏–∞—Ü–∏—è–º –≤—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.

