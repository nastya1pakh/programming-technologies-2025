 # Лабораторная работа №2-2 (Беляев И.С.). Отчёт.

## Задание.
По заданию выбрать свои классы и обучить сверточную нейронную сеть из примера, используя GPU, а затем повысить точность модели.
Провести три обучения для 3 разных тактик пуллинга: пуллинг с помощью шага свёртки stride, макс пуллинг, усредняющий пуллинг.
Сравнить достигнутое качество, время обучения и степень переобучения. Выбрать лучшую конфигурацию. Сохранить модель. Перезапустить среду выполнения - теряются
все текующие переменные.

## Этап №1. Подготовка необходимых модулей и ввод данных.
Первым делом необхоодимо было подготовить все нужные библиотеки. В данной лабораторной работе были использованы такие библиотеки как:
- numpy: Библиотека для работы с многомерными массивами и математическими операциями;
- torch: Основной фреймворк для глубокого обучения; предоставляет тензоры с GPU-поддержкой, автодифференцирование и базовые инструменты для моделей;
- torchsummary: Инструмент для вывода summary модели; показывает слои, параметры и размеры, как в Keras;
- pickle: Стандартная библиотека для сериализации; нужна для чтения pickle-файлов датасета CIFAR-100;
- classification_report: Функция из scikit-learn; генерирует отчёт с precision, recall, f1 для оценки классификации;
- PIL: Библиотека Pillow для изображений; используется для открытия, манипуляции и отображения картинок;
- tqdm: Библиотека для прогресс-баров; оборачивает циклы (эпохи/батчи) для визуализации прогресса обучения;
- clear_output: Функция IPython; очищает вывод в Jupyter/Colab для динамического обновления логов или графиков;
- matplotlib: Библиотека для визуализации; рисует графики loss/accuracy и показывает изображения.

Затем, мы определили видеокарту для того, чтобы нейронная сеть обучалась быстрее:

![Определяем GPU]()

## Этап №2. Подготовка тренировочной и тестовой выборок и их чтение.
На втором этапе мы скачали данные CIFAR100 и сформировали тренировочную и тестовую выборки. В данном варианте, мне попалась выборка с яблоками.

![Картинка с выборки]()

Далее было принято решение создать `Pytorch DataLoader` для удобной манипуляции данными и перейти к следующему этапу.

## Этап №3. Создание Pytorch модели сверточной нейронной сети.
На данном этапе определяются три класса PyTorch-модулей для CNN:
- Normalize нормализует входные изображения (делит на 255, вычитает среднее и делит на std, переставляет оси в NCHW);
- GlobalMaxPool2d выполняет адаптивный max-pooling до 1x1 и flatten (но не используется в модели);
- Cifar100_CNN — основная сеть для CIFAR-100, с последовательностью слоёв в nn.Sequential: нормализация, свёртка с stride=4 для downsampling, ReLU, вторая свёртка, ReLU, AvgPool2d
(или MaxPool2d в комментарии для экспериментов), flatten и линейный слой для классификации на 100 классов (или меньше по CLASSES).
Модель создаётся с hidden_size=32, перемещается на device (GPU/CPU), тестируется на случайном входе (вывод logits), выводится summary архитектуры и сама модель
для отладки, демонстрируя простую CNN с акцентом на разные методы пулинга (stride, avg/max) как в лабораторной задаче.
Вот сводка по модели:

![Описание модели]()

## Этап 4. Обучение модели по эпохам.
На данном этапе предстояло обучить созданную выше модель на загруженных данных.
ЗДесь мы реализовали цикл обучения сверточной нейронной сети (модели из предыдущего фрагмента) на загруженном датасете в PyTorch
на 500 эпохах (EPOCHS=500), с использованием прогресс-бара от tqdm для отслеживания прогресса (общий — 500 эпох * батчей в train). В каждой эпохе модель
переводится в режим обучения (model.train()), и для каждого батча из train DataLoader: данные перемещаются на device (GPU), обнуляются градиенты,
вычисляется forward-pass (outputs = model(inputs)), loss (CrossEntropy, criterion) и accuracy (сравнение argmax), затем backward и шаг оптимизатора
(optimizer.step()); статистика (loss, acc) собирается в список tmp для усреднения/перцентилей. После train модель переводится в eval-режим (model.eval()),
и аналогично вычисляется loss/acc на val (test) DataLoader без градиентов (with torch.no_grad()). Каждые 20 эпох (REDRAW_EVERY=20) очищается вывод
(clear_output), обновляется прогресс-бар, и рисуются два графика Matplotlib: слева — CrossEntropy loss (CCE) для train/val с заполнением между 25-75
перцентилями (как "усы" в boxplot), справа — accuracy (в %); графики обновляются динамически. В конце выводится общее время обучения в секундах (passed).
Это позволяет мониторить сходимость, переобучение (train vs val) и качество в реальном времени, с акцентом на статистику по батчам.
Графики ниже отражают итог обучения модели:

![Графики обучения нейронной сети]()

Затем, было принято решение проверить качество обучения модели по классам на обучающей и тестовой выборке. Ниже представлены результаты данной проверки:

![Проверка обучения модели]()

## Этап №4. Сохранение модели в ONNX.
Для начала необходимо разобраться с тем, что же такое ONNX?
ONNX (Open Neural Network Exchange) — это открытый стандарт и формат для представления моделей машинного обучения (включая глубокие нейронные сети).
Он позволяет экспортировать модели из одного фреймворка (например, PyTorch, TensorFlow, scikit-learn) и запускать их в другом, без переобучения,
обеспечивая интероперабельность между инструментами, аппаратными платформами и средами.
После несложных манипуляций, модель была сохранена в данном формате.
